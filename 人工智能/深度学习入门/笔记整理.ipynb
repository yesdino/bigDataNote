{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95ebf9bf",
   "metadata": {},
   "source": [
    "完整代码 ：https://gitee.com/xu_kai_xuyouqian/tsdemo\n",
    "\n",
    "教程里的版本是1.8 我用的是1.11 好像没什么大区别，都能运行\n",
    "\n",
    "b站视频链接 ： https://www.bilibili.com/video/BV1Wt411C75s?p=27\n",
    "\n",
    "# 1.机器学习介绍\n",
    "\n",
    "## 1.1 深度学习与机器学习的区别\n",
    "\n",
    "### 学习目标\n",
    "\n",
    "- 目标\n",
    "  - 知道机器学习和深度学习的区别\n",
    "- 应用\n",
    "  - 无\n",
    "- 内容预览\n",
    "  - 1.1.1 特征提取方面\n",
    "  - 1.1.2 数据量和计算性能要求\n",
    "  - 1.1.3 算法代表\n",
    "\n",
    "\n",
    "\n",
    "#### 1.1.1特征提取方面\n",
    "\n",
    "- 机器学习的特征工程步骤是要靠手动完成的，而且需要大量领域专业知识\n",
    "- 深度学习通常由多个层组成，它们通常将更简单的模型组合在一起，将数据从一层传递到另一层来构建更复杂的模型。通过训练大量数据自动得出模型，不需要人工特征提取环节。\n",
    "\n",
    "> 深度学习算法试图从数据中学习高级功能，这是深度学习的一个非常独特的部分。因此，减少了为每个问题开发新特征提取器的任务。\n",
    ">\n",
    "> 适合用在难提取特征的图像、语音、自然语言处理领域。\n",
    "\n",
    "#### 1.1.2 数据量和计算性能要求\n",
    "\n",
    "机器学习需要的执行时间远少于深度学习，深度学习参数往往很庞大，需要通过大量数据的多次优化来训练参数。\n",
    "\n",
    "第一、深度学习需要大量的训练数据集\n",
    "\n",
    "第二、训练深度神经网络需要大量的算力\n",
    "\n",
    "\n",
    "可能要花费数天、甚至数周的时间，才能使用数百万张图像的数据集训练出一个深度网络。所以深度学习通常\n",
    "\n",
    "- 需要强大的GPU服务器来进行计算\n",
    "- 全面管理的分布式训练与预测服务――比如谷歌TensorFlow云机器学习平台\n",
    "\n",
    "#### 1.1.3 算法代表\n",
    "\n",
    "- 机器学习\n",
    "  - 朴素贝叶斯\n",
    "  - 决策树等\n",
    "- 深度学习\n",
    "  - 神经网络\n",
    "\n",
    "# 2.`Tensorflow` 框架介绍\n",
    "\n",
    "这里用的是tensorflow 1.11\n",
    "\n",
    "```\n",
    "说明TensorFlow的数据流图结构应用TensorFlow操作图\n",
    "说明会话在TensorFlow程序中的作用\n",
    "应用TensorFlow实现张量的创建、形状类型修改操作应用Variable实现变量op的创建\n",
    "应用Tensorboard实现图结构以及张量值的显示\n",
    "应用tf.train.saver实现TensorFlow的模型保存以及加载应用tf.app.flags实现命令行参数添加和使用\n",
    "应用TensorFlow实现线性回归\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "## 2.1 `Tensorflow`数据流图\n",
    "\n",
    "**学习目标**\n",
    "\n",
    "- ·目标\n",
    "  - 说明`TensorFlow`的数据流图结构\n",
    "- 应用\n",
    "  - 无\n",
    "- 内容预览\n",
    "  - 2.1.1案例:`TensorFlow`实现一个加法运算\n",
    "    - 1代码\n",
    "    - 2 `TensorFlow`结构分析\n",
    "  - 2.1.2数据流图介绍\n",
    "\n",
    "#### 2.1.1 `Tensorflow`实现一个加法运算\n",
    "\n",
    "**代码**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "\n",
    "def tensorflow_demo():\n",
    "    \"\"\"\n",
    "    TensorFlow的基本结构\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 原生python加法运算\n",
    "    a = 2\n",
    "    b = 3\n",
    "    c = a + b\n",
    "    print(\"普通加法运算的结果：\\n\", c)\n",
    "\n",
    "    # TensorFlow实现加法运算\n",
    "    a_t = tf.constant(2)\n",
    "    b_t = tf.constant(3)\n",
    "    c_t = a_t + b_t\n",
    "    print(\"TensorFlow加法运算的结果：\\n\", c_t)\n",
    "\n",
    "    # 开启会话\n",
    "    with tf.Session() as sess:\n",
    "        c_t_value = sess.run(c_t)\n",
    "        print(\"c_t_value:\\n\", c_t_value)\n",
    "\n",
    "    return None\n",
    "```\n",
    "\n",
    "```\n",
    "普通加法运算的结果：\n",
    " 5\n",
    "TensorFlow加法运算的结果：\n",
    " Tensor(\"add:0\", shape=(), dtype=int32)\n",
    "c_t_value:\n",
    " 5\n",
    "```\n",
    "\n",
    "**`Tensorflow`结构分析**\n",
    "\n",
    "`TensorFlow`程序通常被组织成**一个构建图阶段**和**一个执行图阶段**。\n",
    "\n",
    "在构建阶段，数据与操作的执行步骤被描述成一个图。\n",
    "\n",
    "在执行阶段，使用会话执行构建好的图中的操作。\n",
    "\n",
    "- 图和会话︰\n",
    "  - 图:这是`TensorFlow`将计算表示为指令之间的依赖关系的一种表示法。\n",
    "  - 会话: `TensorFlow`跨一个或多个本地或远程设备运行数据流图的机制\n",
    "- 张量:`TensorFlow`中的基本数据对象\n",
    "- 节点:提供图当中执行的操作\n",
    "\n",
    "#### 2.1.2 数据流图介绍\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210404211548.png\"/>\n",
    "\n",
    "`TensorFlow`：\n",
    "        Tensor - 张量 - 数据\n",
    "        Flow - 流动\n",
    "\n",
    "`TensorFlow`是一个采用数据流图(data flow graphs)，用于数值计算的开源框架。\n",
    "节点(Operation)在图中表示数学操作，线(edges)则表示在节点间相互联系的多维数据数组，即张量(tensor) 。\n",
    "\n",
    "### 2.2 图与`TensorBoard`\n",
    "\n",
    "- 目标\n",
    "  - 说明图的基本使用\n",
    "  - 应用`tf.Graph`创建图、`tf.get_default_graph`获取默认图\n",
    "  - 知道开启`TensorBoard`过程\n",
    "  - 知道图当中op的名字以及命名空间\n",
    "- 应用\n",
    "  - 无\n",
    "- 内容预览\n",
    "  - 2.2.1什么是图结构\n",
    "  - 2.2.2图相关操作\n",
    "    - 默认图\n",
    "    - 2创建图\n",
    "  - 2.2.3 `TensorBoard`:可视化学习\n",
    "    - 1数据序列化-events文件\n",
    "    - 2启动`TensorBoard`\n",
    "  - 2.2.4 OP\n",
    "    - 1常见OP\n",
    "    - 2指令名称\n",
    "\n",
    "#### 2.2.1 什么是图结构\n",
    "\n",
    "图包含了一组`tf.Operation`代表的计算单元对象和`tf.Tensor`代赛的计算单元之间流动的数据。\n",
    "\n",
    "图结构：数据（Tensor） + 操作（Operation）\n",
    "\n",
    "#### 2.2.2 图相关操作\n",
    "\n",
    "**1 默认图**\n",
    "通常`TensorFlow`会默认帮我们创建一张图。\n",
    "\n",
    "查看默认图的两种方法:\n",
    "\n",
    "- 通过调用`tf.get_default_graph()`访问，要将操作添加到默认图形中，直接创建OP即可。\n",
    "- op、`sess`都含有graph属性，默认都在一张图中\n",
    "\n",
    "上面我们的代码有了一个加法运算，明明已经有了数据和对应的操作，但是图在哪里呢?\n",
    "\n",
    "其实刚才的运算是使用的默认图\n",
    "\n",
    "代码\n",
    "\n",
    "```python\n",
    "def graph_demo():\n",
    "    \"\"\"\n",
    "    图的演示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # TensorFlow实现加法运算\n",
    "    a_t = tf.constant(2, name=\"a_t\")\n",
    "    b_t = tf.constant(3, name=\"a_t\")\n",
    "    c_t = tf.add(a_t, b_t, name=\"c_t\")\n",
    "    # 方法1： 调用方法\n",
    "    default_g = tf.get_default_graph()\n",
    "    print(default_g)\n",
    "\n",
    "    # 方法2 ： 查看属性\n",
    "    print('a_t的图属性:', a_t.graph)\n",
    "    print('b_t的图属性:', b_t.graph)\n",
    "    print('c_t的图属性:', c_t.graph)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        c_t_value = sess.run(c_t)\n",
    "        print('c_t_value', c_t_value)\n",
    "        print('会话的土属性:', sess.graph)\n",
    "\n",
    "\n",
    "graph_demo()\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "<tensorflow.python.framework.ops.Graph object at 0x000002CC301D4D68>\n",
    "a_t的图属性: <tensorflow.python.framework.ops.Graph object at 0x000002CC301D4D68>\n",
    "b_t的图属性: <tensorflow.python.framework.ops.Graph object at 0x000002CC301D4D68>\n",
    "c_t的图属性: <tensorflow.python.framework.ops.Graph object at 0x000002CC301D4D68>\n",
    "c_t_value 5\n",
    "会话的土属性: <tensorflow.python.framework.ops.Graph object at 0x000002CC301D4D68>\n",
    "```\n",
    "\n",
    "可以看出张量和会话都是在一张图上，内存地址都一样\n",
    "\n",
    "\n",
    "\n",
    "**2 创建图**\n",
    "\n",
    "- 可以通过`tf.Graph（)`自定义创建图\n",
    "- 如果要在这张图中创建OP，典型用法是使用`tf.Graph.as_default()`上下文管理器\n",
    "\n",
    "代码\n",
    "\n",
    "```\n",
    "# 在自几的途中定义数据和操作\n",
    "    with new_g.as_default():\n",
    "        a_new = tf.constant(20)\n",
    "        b_new = tf.constant(30)\n",
    "        c_new = tf.add(a_new,b_new)\n",
    "        print(\"c_new:\",c_new,c_new.graph)\n",
    "```\n",
    "\n",
    "```python\n",
    "def graph_demo():\n",
    "    \"\"\"\n",
    "    图的演示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # TensorFlow实现加法运算\n",
    "    a_t = tf.constant(2, name=\"a_t\")\n",
    "    b_t = tf.constant(3, name=\"b_t\")\n",
    "    c_t = tf.add(a_t, b_t, name=\"c_t\")\n",
    "    # 方法1： 调用方法\n",
    "    default_g = tf.get_default_graph()\n",
    "    print(default_g)\n",
    "\n",
    "    # 方法2 ： 查看属性\n",
    "    print('a_t的图属性:', a_t.graph)\n",
    "    print('b_t的图属性:', b_t.graph)\n",
    "    print('c_t的图属性:', c_t.graph)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        c_t_value = sess.run(c_t)\n",
    "        print('c_t_value', c_t_value)\n",
    "        print('会话的土属性:', sess.graph)\n",
    "\n",
    "    new_g = tf.Graph()\n",
    "    # 在自几的途中定义数据和操作\n",
    "    with new_g.as_default():\n",
    "        a_new = tf.constant(20)\n",
    "        b_new = tf.constant(30)\n",
    "        c_new = tf.add(a_new,b_new)\n",
    "        print(\"c_new:\",c_new,c_new.graph)\n",
    "\n",
    "graph_demo()\n",
    "```\n",
    "\n",
    "```\n",
    "<tensorflow.python.framework.ops.Graph object at 0x00000267E1994E80>\n",
    "a_t的图属性: <tensorflow.python.framework.ops.Graph object at 0x00000267E1994E80>\n",
    "b_t的图属性: <tensorflow.python.framework.ops.Graph object at 0x00000267E1994E80>\n",
    "c_t的图属性: <tensorflow.python.framework.ops.Graph object at 0x00000267E1994E80>\n",
    "c_t_value 5\n",
    "会话的土属性: <tensorflow.python.framework.ops.Graph object at 0x00000267E1994E80>\n",
    "c_new: Tensor(\"Add:0\", shape=(), dtype=int32)\n",
    "<tensorflow.python.framework.ops.Graph object at 0x00000267F9F54F60>\n",
    "```\n",
    "\n",
    "开启了新的图原来的会话里就不能运算c_new 的值了，如果要查看c_new的值：\n",
    "\n",
    "```python\n",
    "with tf.Session(graph=new_g) as new_sess:\n",
    "    c_new_value = new_sess.run(c_new)\n",
    "    print(\"c_new_value:\",c_new_value)\n",
    "```\n",
    "\n",
    "**`TensorFlow`有一个亮点就是，我们能看到自己写的程序的可视化效果，这个功能就是`Tensorboard`。在这里我们先简单介绍一下其基本功能。**\n",
    "\n",
    "#### 2.2.3 可视化学习\n",
    "\n",
    "`TensorFlow`可用于训练大规模深度神经网络所需的计算，使用该工具涉及的计算往往复杂而深奥。为了更方便`TensorFlow`程序的理解、调试与优化，`TensorFlow`提供了`TensorBoard`可视化工具。\n",
    "\n",
    "实现程序可视化过程\n",
    "\n",
    "**1 数据序列化-events文件**\n",
    "`TensorBoard`通过读取Te`nsorFlow的`事件文件来运行，需要将数据生成一个序列化的`Summary protobuf` 对象。\n",
    "\n",
    "```python\n",
    " with tf.Session() as sess:\n",
    "        c_t_value = sess.run(c_t)\n",
    "        print('c_t_value', c_t_value)\n",
    "        print('会话的土属性:', sess.graph)\n",
    "        # 1) 把图写入本地生成events 文件\n",
    "        tf.summary.FileWriter(\"./tmp/summary\", graph=sess.graph)\n",
    "```\n",
    "\n",
    "这将在指定目录中生成一个event文件，其名称格式如下:\n",
    "\n",
    "```\n",
    "events.out.tfevents.{timestamp} . {hostname}\n",
    "```\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210404232826.png\"/>\n",
    "\n",
    "**2启动`TensorBoard`**\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210404234257.png\"/>\n",
    "\n",
    "这是我的路径，然后在终端输入命令\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210404234402.png\"/>\n",
    "\n",
    "注意我打开的终端位置和后面输入的命令\n",
    "\n",
    "`tensorboard --logdir=\"tmp/summary\"`\n",
    "\n",
    "在浏览器中打开`TensorBoard`的图页面`127.0.0.1:6006`，会看到与以下图形类似的图,在GRAPHS模块我们可以看到以下图结构\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210404235022.png\"/>\n",
    "\n",
    "注意：\n",
    "\n",
    "这里终端提示你打开\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210404234710.png\"/>\n",
    "\n",
    "这个网址，但是这个进不去，一定要进`127.0.0.1:6006`\n",
    "\n",
    "#### 2.2.4 OP\n",
    "\n",
    "**1 常见op**\n",
    "\n",
    "哪些是op\n",
    "\n",
    "| 类型           | 实例                                                 |\n",
    "| -------------- | ---------------------------------------------------- |\n",
    "| 标量运算       | add,sub, mul, div, exp, log, greater, less, equal    |\n",
    "| 向量运算       | concat, slice, splot, constant, rank, shape, shuffle |\n",
    "| 矩阵运算       | matmul,matrixinverse, matrixdateminant               |\n",
    "| 带状态的运算   | Variable, assgin, assginadd                          |\n",
    "| 神经网络组件   | softmax,sigmoid, relu,convolution,max_pool           |\n",
    "| 存储，恢复     | Save, Restroe                                        |\n",
    "| 队列及同步运算 | Enqueue, Dequeue,MutexAcquire,MutexRelease           |\n",
    "| 控制流         | Merge,Switch, Enter,Leave,Nextlteration              |\n",
    "\n",
    "接下来我们要区分一下操作函数和操作对象\n",
    "\n",
    "`tf.constant()`就是一个操作函数，传入参数运行以后回产生一个张量，就是操作对象\n",
    "\n",
    "| 操作函数                         | 操作对象                                                  |\n",
    "| -------------------------------- | --------------------------------------------------------- |\n",
    "| tf.constant(Tensor对象)          | 输入Tensor对象 -Const-输出 Tensor对象                     |\n",
    "| tf.add(Tensor对象1, Tensor对象2) | 输入Tensor对象1, Tensor对象2 - Add对象 - 输出 Tensor对象3 |\n",
    "\n",
    ">一个操作对象(Operation)）是`TensorFlow`图中的一个节点,可以接收0个或者多个输入Tensor,并且可以输出0个或者多个Tensor，Operation对象是通过op构造函数(如`tf.matmul()`）创建的。\n",
    ">\n",
    ">例如:` c = tf.matmul(a, b)`创建了一个Operation对象，类型为 `MatMul`类型,它将张量a, b作为输入，c作为输出，并且输出数据，打印的时候也是打印的数据。其中`tf.matmul)`是函数，在执行`matmul`函数的过程中会通过`MatMul`类创建一个与之对应的对象\n",
    "\n",
    "**2指令名称**\n",
    "`tf.Graph`对象为其包含的` tf.Operation`对象定义的一个命名空间。`TensorFlow` 会自动为图中的每个指令选择一个唯一名称，用户也可以指定描述性名称，使程序阅读起来更轻松。我们可以以以下方式改写指令名称\n",
    "\n",
    "- 每个创建新的`tf.Operation`或返回新的`tf.Tensor`的 `API`函数可以接受可选的name 参数。\n",
    "\n",
    "例如，`tf.constant(42.0, name=\"answer\")`创建了一个名为“\"answer”的新`tf.Operation`并返回一个名为`“answer:O”`的`tf.Tenso`r。如果默认图已包含名为\"answer”的指令，则`TensorFlow` 会在名称上附加“1\"、\"\"2”等字符，以便让名称具有唯一性。\n",
    "\n",
    "- 当修改好之后，我们在`Tensorboard`显示的名字也会被修改\n",
    "\n",
    "```python\n",
    "a = tf.constant(1)\n",
    "b = tf.constant(2, name='b')\n",
    "print('a=', a)\n",
    "print('b=', b)\n",
    "```\n",
    "\n",
    "```\n",
    "a= Tensor(\"Const:0\", shape=(), dtype=int32) # 这个我们没有指定指令名称，他是通过contant函数创建的，就叫const：0\n",
    "b= Tensor(\"b:0\", shape=(), dtype=int32)  # 这个叫b：0\n",
    "```\n",
    "\n",
    "### 2.3 会话\n",
    "\n",
    "**1. 创建会话**\n",
    "\n",
    "一个运行`TensorFlow operation`的类。会话包含以下两种开启方式\n",
    "\n",
    "- `tf.Session`:用于完整的程序当中\n",
    "- `tf.InteractiveSession`:用于交互式上下文中的`TensorFlow `,例如shell\n",
    "\n",
    ">1 `TensorFlow`使用`tf.Session`类来表示客户端程序（通常为Python程序，但也提供了使用其他语言的类似接口)与C++运行时之间的连接\n",
    ">\n",
    ">2 `tf.Session`对象使用分布式`TensorFlow`运行时提供对本地计算机中的设备和远程设备的访问权限。\n",
    "\n",
    "\n",
    "\n",
    "**1` __init__(target=\", graph=None, config=None)`**\n",
    "会话可能拥有的资源，如`tf.Variable`，`tf.QueueBase`和`tf.ReaderBase`。当这些资源不再需要时，释放这些资源非常重要。因此，需要调用`tf.Sessioh.close`会话中的方法，或将会话用作上下文管理器。以下两个例子作用是一样的(直白一点就是打开会话用完必须要关闭，就像文件读写那样):\n",
    "\n",
    "```python\n",
    "def session_demo():\n",
    "    a_t = tf.constant(10)\n",
    "    b_t = tf.constant(20)\n",
    "    # 不提倡直接用+ - 符号直接运算\n",
    "    # 推荐用tensorflow 提供的函数进行运算\n",
    "    # c_t = a_t + b_t\n",
    "    c_t = tf.add(a_t, b_t)\n",
    "    print(\"tensorflow实现加法运算:\\n\", c_t)\n",
    "\n",
    "    # 开启会话\n",
    "    # 传统的定义会话\n",
    "    sess = tf.Session()\n",
    "    sum_t = sess.run(c_t)\n",
    "    print(\"sum_t:\\n\", sum_t)\n",
    "    sess.close()\n",
    "    print('------分割线-------------')\n",
    "    # 用上下文管理开启会话\n",
    "    with tf.Session() as sess:\n",
    "        # 同时执行多个tensor\n",
    "        print(sess.run([a_t, b_t, c_t]))\n",
    "        # 也可以用eval 查看值\n",
    "        print('用eval查看计算的值', a_t.eval())\n",
    "        print('会话的属性：\\n', sess.graph)\n",
    "\n",
    "\n",
    "session_demo()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "tensorflow实现加法运算:\n",
    " Tensor(\"Add:0\", shape=(), dtype=int32)\n",
    "sum_t:\n",
    " 30\n",
    "------分割线-------------\n",
    "[10, 20, 30]\n",
    "用eval查看计算的值 10\n",
    "会话的属性：\n",
    " <tensorflow.python.framework.ops.Graph object at 0x00000121D7594F60>\n",
    "```\n",
    "\n",
    "- target:如果将此参数留空（默认设置)，会话将仅使用本地计算机中的设备。可以指定grpc:/l网址，以便指定TensorFlow服务器的地址，这使得会话可以访问该服务器控制的计算机上的所有设备。\n",
    "- graph:默认情况下，新的tf.Session将绑定到当前的默认图。\n",
    "- config:此参数允许您指定一个tf.ConfigProto 以便控制会话的行为。例如，ConfigProto协议用于打印设备使用信息\n",
    "\n",
    "我在上一段代码中加入config设置\n",
    "\n",
    "```python\n",
    "with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,\n",
    "                                      log_device_placement=True)) as sess:\n",
    "        # 同时执行多个tensor\n",
    "        print(sess.run([a_t, b_t, c_t]))\n",
    "        # 也可以用eval 查看值\n",
    "        print('用eval查看计算的值', a_t.eval())\n",
    "        print('会话的属性：\\n', sess.graph)\n",
    "```\n",
    "\n",
    "```\n",
    "tensorflow实现加法运算:\n",
    " Tensor(\"Add:0\", shape=(), dtype=int32)\n",
    "sum_t:\n",
    " 30\n",
    "------分割线-------------\n",
    "Device mapping: no known devices.\n",
    "c_t: (Add): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "Add: (Add): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "a_t: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "b_t: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "Const: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "Const_1: (Const): /job:localhost/replica:0/task:0/device:CPU:0\n",
    "[10, 20, 30]\n",
    "用eval查看计算的值 10\n",
    "会话的属性：\n",
    " <tensorflow.python.framework.ops.Graph object at 0x000002B45C0B4F28>\n",
    "```\n",
    "\n",
    "现在运行结果中显示出了我的设备信息\n",
    "\n",
    "**2 feed操作**\n",
    "placeholder提供占位符，run时候通过feed_dict指定参数\n",
    "\n",
    "可以理解为先声明一个变量，然后后面再赋值\n",
    "\n",
    "```python\n",
    "def session_run_demo():\n",
    "    a = tf.placeholder(tf.float32)\n",
    "    b = tf.placeholder(tf.float32)\n",
    "\n",
    "    sum_ab = tf.add(a, b)\n",
    "    print(\"sum_ab:\", sum_ab)\n",
    "\n",
    "    # 开启会话\n",
    "    with tf.Session() as sess:\n",
    "        print('占位符结果：', sess.run(sum_ab, feed_dict={a: 3.0, b: 4.0}))\n",
    "\n",
    "\n",
    "session_run_demo()\n",
    "\n",
    "```\n",
    "\n",
    "### 2.4 张量\n",
    "\n",
    "#### 2.4.1 张量\n",
    "\n",
    "标量：一个数值（0阶张量）\n",
    "\n",
    "向量：一维数组（1阶张量）\n",
    "\n",
    "矩阵：二维数组（2阶张量）\n",
    "\n",
    "这三个都可以用张量表示，同时张量也可以扩展到3,4...n阶\n",
    "\n",
    "**1  张量的类型**\n",
    "\n",
    "| 数据类型     | Python 类型  | 描述                                               |\n",
    "| ------------ | ------------ | -------------------------------------------------- |\n",
    "| DT_FLOAT     | tf.float32   | 32 位浮点数.                                       |\n",
    "| DT_DOUBLE    | tf.float64   | 64 位浮点数.                                       |\n",
    "| DT_INT64     | tf.int64     | 64 位有符号整型.                                   |\n",
    "| DT_INT32     | tf.int32     | 32 位有符号整型.                                   |\n",
    "| DT_INT16     | tf.int16     | 16 位有符号整型.                                   |\n",
    "| DT_INT8      | tf.int8      | 8 位有符号整型.                                    |\n",
    "| DT_UINT8     | tf.uint8     | 8 位无符号整型.                                    |\n",
    "| DT_STRING    | tf.string    | 可变长度的字节数组.每一个张量元素都是一个字节数组. |\n",
    "| DT_BOOL      | tf.bool      | 布尔型.                                            |\n",
    "| DT_COMPLEX64 | tf.complex64 | 由两个32位浮点数组成的复数:实数和虚数.             |\n",
    "| DT_QINT32    | tf.qint32    | 用于量化Ops的32位有符号整型.                       |\n",
    "| DT_QINT8     | tf.qint8     | 用于量化Ops的8位有符号整型.                        |\n",
    "| DT_QUINT8    | tf.quint8    | 用于量化Ops的8位无符号整型.                        |\n",
    "\n",
    "**2  张量的阶**\n",
    "\n",
    "| 阶   | 数学实例 | python     | 例子                   |\n",
    "| ---- | -------- | ---------- | ---------------------- |\n",
    "| 0    | 纯量     | 只有大小   | s=123                  |\n",
    "| 1    | 向量     | 大小和方向 | v = [1,2]              |\n",
    "| 2    | 矩阵     | 数据表     | m= [[1,2],[3,4],[5,6]] |\n",
    "| 3    | 3阶张量  | 数据立体   | ....                   |\n",
    "| n    | n阶      |            |                        |\n",
    "\n",
    "代码演示\n",
    "\n",
    "```python\n",
    "def tensor_demo():\n",
    "    \"\"\"\n",
    "    张量的演示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tensor1 = tf.constant(4.0)\n",
    "    tensor2 = tf.constant([1, 2, 3, 4])\n",
    "    linear_squares = tf.constant([[4], [9], [16], [25]], dtype=tf.int32)\n",
    "\n",
    "    print(\"tensor1:\\n\", tensor1)\n",
    "    print(\"tensor2:\\n\", tensor2)\n",
    "    print(\"linear_squares_before:\\n\", linear_squares)\n",
    "\n",
    "tensor_demo()\n",
    "```\n",
    "\n",
    "```\n",
    "tensor1:\n",
    " Tensor(\"Const_2:0\", shape=(), dtype=float32)\n",
    "tensor2:\n",
    " Tensor(\"Const_3:0\", shape=(4,), dtype=int32)\n",
    "linear_squares_before:\n",
    " Tensor(\"Const_4:0\", shape=(4, 1), dtype=int32)\n",
    "```\n",
    "\n",
    "如果传入的是整数默认int32，浮点数默认float32\n",
    "\n",
    "#### 2.4.2 创建张量的指令\n",
    "\n",
    "**1 . 固定值张量**\n",
    "\n",
    "**`tf.zeros(shape, dtype=tf.float32, name=None)`**\n",
    "创建所有元素设置为零的张量。此操作返回一个dtype具有形状shape和所有元素设置为零的类型的张量。\n",
    "\n",
    "**`tf.zeros_like(tensor, dtype=None, name=None)`**\n",
    "给tensor定单张量()，此操作返回tensor与所有元素设置为零相同的类型和形状的张量。\n",
    "\n",
    "**`tf.ones(shape, dtype=tf.float32, name=None)`**\n",
    "创建一个所有元素设置为1的张量。此操作返回一个类型的张量,dtype形状shape和所有元素设置为1。\n",
    "\n",
    "**`tf.ones_like(tensor, dtype=None, name=None)`**\n",
    "给tensor定单张量()，此操作返回tensor与所有元素设置为1相同的类型和形状的张量。\n",
    "\n",
    "**`tf.fill(dims, value, name=None)`**\n",
    "创建一个填充了标量值的张量。此操作创建一个张量的形状dims并填充它value。\n",
    "\n",
    "**`tf.constant(value, dtype=None, shape=None, name='Const')`**\n",
    "创建一个常数张量。\n",
    "\n",
    "**2  . 随机值张量**\n",
    "\n",
    "一般我们经常使用的随机数函数Math.random() 产生的是服从均匀分布的随机数，能够模拟等概率出现的况，例如扔一个骰子，1到6点的概率应该相等，但现实生活中更多的随机现象是符合正态分布的，例如20岁成年人的体重分布等。\n",
    "\n",
    "假如我们在制作一个游戏，要随机设定许许多多NPC的身高，如果还用Math.random()，生成从140到22之间的数字，就会发现每个身高段的人数是一样多的，这是比较无趣的，这样的世界也与我们习惯不同，玛实应该是特别高和特别矮的都很少，处于中间的人数最多，这就要求随机函数符合正态分布。\n",
    "\n",
    "`tf.truncated_normal[shape, mean=0.0, stddev=1.0, ctype=tf.float32, seed=None, name=None)`\n",
    "从截断的正态分布中输出随机值，和 tf.random_normal)一样，但是所有数字都不超过两个标准差\n",
    "\n",
    "`tf.random_normal(shape, mean=0.0, stddev=1.0, dtype=tf.float32,seed=None, name=None)`\n",
    "从正态分布中输出随机值，由随机正态分布的数字组成的矩阵\n",
    "\n",
    "\n",
    "\n",
    "#### 2.4.3 张量的变换\n",
    "\n",
    "**1 类型改变**\n",
    "\n",
    "```\n",
    "tf.string_to_number(string_tensor, out_type=None, name=None)\n",
    "tf.to_double(x, name='ToDouble')\n",
    "tf.to_float(x, name='ToFloat')\n",
    "tf.to_bfloat16(x, name='ToBFloat16')\n",
    "tf.to_int32(x, name='Tolnt32')\n",
    "tf.to_int64(x, name='Tolnt64')\n",
    "tf.cast(x, dtype, name=None) # 可以理解成np.astype\n",
    "```\n",
    "\n",
    "```python\n",
    "def tensor_demo():\n",
    "    \"\"\"\n",
    "    张量的演示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    tensor1 = tf.constant(4.0)\n",
    "    tensor2 = tf.constant([1, 2, 3, 4])\n",
    "    linear_squares = tf.constant([[4], [9], [16], [25]], dtype=tf.int32)\n",
    "\n",
    "    print(\"tensor1:\\n\", tensor1)\n",
    "    print(\"tensor2:\\n\", tensor2)\n",
    "    print(\"linear_squares_before:\\n\", linear_squares)\n",
    "    tensor3 = tf.cast(linear_squares,tf.float32)\n",
    "    print('tensor3:\\n',tensor3)\n",
    "    print(\"linear_squares_after:\\n\", linear_squares)\n",
    "\n",
    "\n",
    "tensor_demo()\n",
    "```\n",
    "\n",
    "```\n",
    "tensor1:\n",
    " Tensor(\"Const_2:0\", shape=(), dtype=float32)\n",
    "tensor2:\n",
    " Tensor(\"Const_3:0\", shape=(4,), dtype=int32)\n",
    "linear_squares_before:\n",
    " Tensor(\"Const_4:0\", shape=(4, 1), dtype=int32)\n",
    "tensor3:\n",
    " Tensor(\"Cast:0\", shape=(4, 1), dtype=float32)\n",
    "linear_squares_after:\n",
    " Tensor(\"Const_4:0\", shape=(4, 1), dtype=int32)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "**2 形状改变**\n",
    "\n",
    "\n",
    "\n",
    "`TensorFlow`的张量具有两种形状变换，动态形状和静态形状\n",
    "\n",
    "- tf.reshape\n",
    "- tf.set_shape\n",
    "\n",
    "关于动态形状和静态形状必须符合以下规则\n",
    "\n",
    "- 静态形状\n",
    "  - 转换静态形状的时候，1-D到1-D，2-D到2-D，不能跨阶数改变形状（比如原来形状是[None,None],可以改成[3,4],不能改成[2,2,2]）\n",
    "  - 对于已经固定的张量的静态形状的张量，不能再次设置静态形状\n",
    "- 动态形状\n",
    "  - tf.reshape)动态创建新张量时，张量的元素个数必须匹配\n",
    "\n",
    "**静态形状代码**\n",
    "\n",
    "```python\n",
    "a_p = tf.placeholder(dtype=tf.float32, shape=[None, None])\n",
    "b_p = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n",
    "c_p = tf.placeholder(dtype=tf.float32, shape=[3, 2])\n",
    "print('a_p静态形状:', a_p.get_shape())\n",
    "print('b_p静态形状:', b_p.get_shape())\n",
    "print('c_p静态形状,已经固定:', c_p.get_shape())\n",
    "# 形状更新\n",
    "a_p.set_shape([2, 3])\n",
    "\n",
    "# 静态形状已经固定部分就不能修改了\n",
    "b_p.set_shape([3, 10])\n",
    "print('a_p修改后的静态形状:', a_p.get_shape())\n",
    "print('b_p修改后的静态形状:', b_p.get_shape())\n",
    "c_p.set_shape([2, 3])\n",
    "print('c_p修改后的静态形状：', c_p.get_shape())\n",
    "```\n",
    "\n",
    "```\n",
    "a_p静态形状: (?, ?)\n",
    "b_p静态形状: (?, 10)\n",
    "c_p静态形状,已经固定: (3, 2)\n",
    "a_p修改后的静态形状: (2, 3)\n",
    "b_p修改后的静态形状: (3, 10)\n",
    "最后一次修改形状会报错\n",
    "ValueError: Dimension 0 in both shapes must be equal, but are 3 and 2. Shapes are [3,2] and [2,3].\n",
    "```\n",
    "\n",
    "**动态形状代码**\n",
    "\n",
    "```python\n",
    "c_p = tf.placeholder(dtype=tf.float32, shape=[3, 2, 3])\n",
    "\n",
    "print('c_p静态形状:', c_p.get_shape())\n",
    "# 形状更新\n",
    "new_c_p = tf.reshape(c_p, shape=[3, 3, 2])\n",
    "print('c_p更新后的静态形状:', c_p.get_shape())\n",
    "print('new_c_p的静态形状:', new_c_p.get_shape())\n",
    "```\n",
    "\n",
    "```\n",
    "c_p静态形状: (3, 2, 3)\n",
    "c_p更新后的静态形状: (3, 2, 3)\n",
    "new_c_p的静态形状: (3, 3, 2)\n",
    "不会改变原来的tensor，而是返回新的tensor\n",
    "```\n",
    "\n",
    "#### 2.4.4 张量的数学运算\n",
    "\n",
    "- 算术运算符\n",
    "- 基本数学函数\n",
    "- 矩阵运算\n",
    "- reduce操作\n",
    "- 序列索引操作\n",
    "\n",
    "> 详细请参考:\n",
    "> https://tensorflow.google.cn/versions/r1.15/api_docs/python/tf/math\n",
    ">\n",
    "> 这些API使用，我们在使用的时候介绍，具体参考文档\n",
    "\n",
    "比如reduce_mean 的使用\n",
    "\n",
    "```python\n",
    "x = tf.constant([[1., 1.], [2., 2.]])\n",
    "tf.reduce_mean(x)  # 1.5 求所有数字的平均\n",
    "tf.reduce_mean(x, 0)  # [1.5, 1.5] # 求第一个轴上的平均\n",
    "tf.reduce_mean(x, 1)  # [1.,  2.] # 求第二个轴上的平均\n",
    "```\n",
    "\n",
    "### 2.5 变量\n",
    "\n",
    "TensorFlow变量是表示程序处理的共享持久状态的最佳方法。变量通过tf.VariableOP类进行操作。变量的特点∶\n",
    "\n",
    "- 存储持久化\n",
    "- 可修改值\n",
    "- 可指定被训练\n",
    "\n",
    "变量是用来保存深度学习的模型参数\n",
    "\n",
    "#### 2.5.1 创建变量\n",
    "\n",
    "`tf.Variable(initial_value=None,trainable=True,collections=None,name=None)`\n",
    "\n",
    "- initial_value:初始化的值\n",
    "- trainable:是否被训练\n",
    "- collections:新变量将添加到列出的图的集合中collections，默认为[GraphKeys.GLOBAL_VARIABLES]，如果trainable是True变量也被添加到图形集合GraphKeys.TRA INABLE_VARIABLES\n",
    "- 变量需要显式初始化，才能运行值\n",
    "\n",
    "```python\n",
    "def variable_demo():\n",
    "    \"\"\"\n",
    "    变量的演示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 创建一个命名空间 创建变量 \n",
    "    with tf.variable_scope(\"my_scope\"):\n",
    "        a = tf.Variable(initial_value=50)\n",
    "        b = tf.Variable(initial_value=40)\n",
    "    # 创建另一个命名空间\n",
    "    with tf.variable_scope(\"your_scope\"):\n",
    "        c = tf.add(a, b)\n",
    "    print(\"a:\\n\", a)\n",
    "    print(\"b:\\n\", b)\n",
    "    print(\"c:\\n\", c)\n",
    "\n",
    "    # 初始化变量\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # 开启会话\n",
    "    with tf.Session() as sess:\n",
    "        # 运行初始化\n",
    "        sess.run(init)\n",
    "        a_value, b_value, c_value = sess.run([a, b, c])\n",
    "        print(\"a_value:\\n\", a_value)\n",
    "        print(\"b_value:\\n\", b_value)\n",
    "        print(\"c_value:\\n\", c_value)\n",
    "```\n",
    "\n",
    "```\n",
    "a:\n",
    " <tf.Variable 'my_scope/Variable:0' shape=() dtype=int32_ref>\n",
    "b:\n",
    " <tf.Variable 'my_scope/Variable_1:0' shape=() dtype=int32_ref>\n",
    "c:\n",
    " Tensor(\"your_scope/Add:0\", shape=(), dtype=int32)\n",
    "a_value:\n",
    " 50\n",
    "b_value:\n",
    " 40\n",
    "c_value:\n",
    " 90\n",
    "\n",
    "```\n",
    "\n",
    "### 2.6 高级API\n",
    "\n",
    "#### 2.6.1 基础api\n",
    "\n",
    "**1 tf.app**\n",
    "这个模块相当于为TensorFlow进行的脚本提供一个main 函数入口，可以定义脚本运行的 flags。\n",
    "**2 tf.image**\n",
    "TensorFlow的图像处理操作。主要是一些颜色变换、变形和图像的编码和解码。\n",
    "**3 tf.gfile**\n",
    "\n",
    "提供文件操作模块\n",
    "\n",
    "**4 tf.summary**\n",
    "用来生成TensorBoard可用的统计日志，目前Summary主要提供了4种类型;audio、image、histogram.scalar\n",
    "\n",
    "**5 tf.python_io**\n",
    "用来读写TFRecords文件\n",
    "**6 tf.train**\n",
    "\n",
    "这个模块提供了一些训练器，与tf.nn组合起来，实现一些网络的优化计算。\n",
    "**7 tf.nn**\n",
    "这个模块提供了一些构建神经网络的底层函数。TensorFlow构建网络的核心模块。其中包含了添加各种层的函数，比如添加卷积层、池化层等。\n",
    "\n",
    "#### 2.6.2高级api\n",
    "\n",
    "**1 tf.keras**\n",
    "Keras本来是一个独立的深度学习库，tensorflow将其学习过来，增加这部分模块在于快速构建模型。\n",
    "**2 tf.layers**\n",
    "高级API，以更高级的概念层来定义一个模型。尖似tf.Keras \n",
    "\n",
    "\n",
    "\n",
    "**3 tf.contrib**\n",
    "tf.contrib.layers提供够将计算图中的网络层、正则化、摘要操作、是构建计算图的高级操作，但是tf.contrib包含不稳定和实验代码，有可能以后API会改变。\n",
    "**4 tf.estimator**\n",
    "一个Estimator相当于Model + Training + Evaluate 的合体。在模块中，已经实现了几种简单的分类器和回归器，包括:Baseline，Learning 和 DNN。这里的 DNN的网络，只是全连接网络，没有提供卷积之类的。\n",
    "\n",
    "关于tensorflowapi的图示\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210405142142.png\"/>\n",
    "\n",
    "### 2.7 实现线性回归\n",
    "\n",
    "#### 2.7.1线性回归原理复习\n",
    "\n",
    "根据数据建立回归模型,`w1x1+w2x2+......b = y`，通过真实值与预测值之间建立误差，使用梯度下降优化得到损失最小对应的权重和偏置。最终确定模型的权重和偏置参数。最后可以用这些参数进行预测。\n",
    "\n",
    "\n",
    "\n",
    "#### 2.7.2案例:实现线性回归的训练\n",
    "\n",
    "**1案例确定**\n",
    "\n",
    "- 假设随机指定100个点，只有一个特征\n",
    "- 数据本身的分布为y= 0.8*x+0.7\n",
    "\n",
    ">  这里将数据分布的规律确定，是为了使我们训练出的参数跟真实的参数（即0.8和0.7)比较是否训练准确\n",
    "\n",
    "**2 API**\n",
    "\n",
    "\n",
    "\n",
    "<u>*运算*</u>\n",
    "\n",
    "\n",
    "\n",
    "- 矩阵运算\n",
    "  -  tf.matmul(x, w)\n",
    "- 平方\n",
    "  - tf.square(error)\n",
    "- 均值\n",
    "  -  tf.reduce_mean(error)\n",
    "\n",
    "<u>*梯度下降优化*</u>\n",
    "\n",
    "- tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  - 梯度下降优化\n",
    "  - learning_rate:学习率，一般为0~1之间比较小的值\n",
    "  - method:\n",
    "    - minimize(loss)\n",
    "  - return:梯度下降op\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def linear_regression():\n",
    "    '''\n",
    "    实现一个线性回归\n",
    "    '''\n",
    "\n",
    "    # 1). 准备数据\n",
    "    X = tf.random_normal(shape=[100, 1], name=\"feature\")\n",
    "    y_true = tf.matmul(X, [[0.8]]) + 0.7\n",
    "\n",
    "    # 2). 构造模型\n",
    "    # 用变量定义模型参数\n",
    "    weights = tf.Variable(initial_value=tf.random_normal(shape=[1, 1]), name='weight')\n",
    "    bias = tf.Variable(initial_value=tf.random_normal(shape=[1, 1]), name=\"Bias\")\n",
    "    # X [100,1]\n",
    "    y_predict = tf.matmul(X, weights) + bias\n",
    "\n",
    "    # 3). 构造损失函数\n",
    "    error = tf.reduce_mean(tf.square(y_predict - y_true))\n",
    "    # 4). 优化器梯度下降\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(error)\n",
    "    # 显式初始化变量\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    #  开启会话\n",
    "    with tf.Session() as sess:\n",
    "        # 初始化变量\n",
    "        sess.run(init)\n",
    "        # 查看初始化模型参数之后的值\n",
    "        print(\"训练前模型参数为:w:%f.b:%f,loss:%f\" % (weights.eval(), bias.eval(), error.eval()))\n",
    "\n",
    "        # 开始驯良\n",
    "        for i in range(100):\n",
    "            sess.run(optimizer)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(\"第%d次训练后模型参数：w=%f,b=%f,loss=%f\" % (i + 1, weights.eval(), bias.eval(),error.eval()))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "linear_regression()\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "训练前模型参数为:w:0.721496.b:0.444022,loss:0.067662\n",
    "第10次训练后模型参数：w=0.734411,b=0.490444,loss=0.044472\n",
    "第20次训练后模型参数：w=0.744739,b=0.528224,loss=0.031570\n",
    "第30次训练后模型参数：w=0.756377,b=0.559797,loss=0.020845\n",
    "第40次训练后模型参数：w=0.764220,b=0.585331,loss=0.014033\n",
    "第50次训练后模型参数：w=0.771134,b=0.606354,loss=0.008801\n",
    "第60次训练后模型参数：w=0.776506,b=0.623496,loss=0.006291\n",
    "第70次训练后模型参数：w=0.780643,b=0.637487,loss=0.004517\n",
    "第80次训练后模型参数：w=0.784405,b=0.649035,loss=0.002682\n",
    "第90次训练后模型参数：w=0.787279,b=0.658381,loss=0.001830\n",
    "第100次训练后模型参数：w=0.789829,b=0.666036,loss=0.001318\n",
    "\n",
    "```\n",
    "\n",
    "**学习率的设置、步数的设置与梯度爆炸**\n",
    "学习率越大，训练到较好结果的步数越小;学习率越小，训练到较好结果的步数越大。\n",
    "\n",
    "但是学习过大会出现梯度爆炸现象。关于梯度爆炸/梯度消失?\n",
    "在极端情况下，权重的值变得非常大,以至于溢出，导致 NaN值如何解决梯度爆炸问题(深度神经网络当中更容易出现)\n",
    "\n",
    "```\n",
    "1、重新设计网络\n",
    "2、调整学习率\n",
    "3、使用梯度截断在训练过程中检查和限制梯度的大小\n",
    "4、使用激活函数\n",
    "```\n",
    "\n",
    "如果不想训练某个参数，比如不想训练w\n",
    "\n",
    "`weights = tf.Variable(initial_value=tf.random_normal(shape=[1, 1]), name='weight',trainable=False)`\n",
    "\n",
    "可以看到w参数不会更新\n",
    "\n",
    "```\n",
    "第930次训练后模型参数：w=2.242931,b=0.696321,loss=2.261457\n",
    "第940次训练后模型参数：w=2.242931,b=0.699000,loss=2.239960\n",
    "第950次训练后模型参数：w=2.242931,b=0.701000,loss=2.051921\n",
    "第960次训练后模型参数：w=2.242931,b=0.686143,loss=2.046947\n",
    "第970次训练后模型参数：w=2.242931,b=0.694059,loss=1.899806\n",
    "第980次训练后模型参数：w=2.242931,b=0.697114,loss=1.998867\n",
    "第990次训练后模型参数：w=2.242931,b=0.692461,loss=1.933030\n",
    "第1000次训练后模型参数：w=2.242931,b=0.703330,loss=2.646688\n",
    "```\n",
    "\n",
    "### 2.7.1 增加其他功能\n",
    "\n",
    "**1 . 增加变量显式**\n",
    "\n",
    "- 变量Tensorboard显式\n",
    "- 增加命名空间\n",
    "- 模型保存和加载\n",
    "- 命令行参数设置\n",
    "\n",
    "**目的:在TensorBoard当中观察模型的参数、损失值等变量值的变化·** \n",
    "\n",
    "- 1、收集变量\n",
    "  - tf.summary.scalar(name=\",tensor)收集对于损失函数和准确率等单值变量,name为变量的名字,tensor为值\n",
    "  - tf.summary.histogram(name=\",tensor)收集高维度的变量参数\n",
    "  - tf.summary.image(name=\",tensor)收集输入的图片张量能显示图片\n",
    "- 2、合并变量写入事件文件\n",
    "  - merged = tf.summary.merge_all(）\n",
    "  - 运行合并: summary = sess.run(merged)，每次迭代都需运行\n",
    "  - 添加:FileWriter.add_summary(summary,i),i表示第几次的值eixin_4451731\n",
    "\n",
    "**2增加命名空间**\n",
    "\n",
    "是代码结构更加清晰，Tensorboard图结构清楚\n",
    "`with tf.variable_scope( \"lr_model\"\" ):`\n",
    "\n",
    "**3模型的保存与加载**\n",
    "\n",
    "- tf.train.Saver(var_list=None,max_to_keep=5)\n",
    "  - 保存和加载模型（保存文件格式: checkpoint文件)\n",
    "  - var_list:指定将要保存和还原的变量。它可以作为一个dict或一个列表传递.\n",
    "  - max_to_keep:指示要保留的最近检查点文件的最大数量。创建新文件时，会删除较旧的文件。如果无或0，则保留所有检查点文件。默认为 5(即保留最新的5个检查点文件。)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "使用\n",
    "\n",
    "```\n",
    "例如:\n",
    "指定目录+模型名字\n",
    "saver.save(sess,' /tmp/ckpt/test/myregression.ckpt ')\n",
    "saver.restore(sess,'/tmp/ckpt/test/myregression.ckpt ' )\n",
    "\n",
    "```\n",
    "\n",
    "代码：\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def linear_regression():\n",
    "    '''\n",
    "    实现一个线性回归\n",
    "    '''\n",
    "    with tf.variable_scope('preparedata'):\n",
    "        # 1). 准备数据\n",
    "        X = tf.random_normal(shape=[100, 1], name=\"feature\")\n",
    "        y_true = tf.matmul(X, [[0.8]]) + 0.7\n",
    "    with tf.variable_scope('create_model'):\n",
    "        # 2). 构造模型\n",
    "        # 用变量定义模型参数\n",
    "        weights = tf.Variable(initial_value=tf.random_normal(shape=[1, 1]), name='weight',)\n",
    "        bias = tf.Variable(initial_value=tf.random_normal(shape=[1, 1]), name=\"Bias\")\n",
    "        # X [100,1]\n",
    "        y_predict = tf.matmul(X, weights) + bias\n",
    "    with tf.variable_scope('loss'):\n",
    "        # 3). 构造损失函数\n",
    "        error = tf.reduce_mean(tf.square(y_predict - y_true))\n",
    "    with tf.variable_scope('optimezer'):\n",
    "        # 4). 优化器梯度下降\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(error)\n",
    "\n",
    "    # 收集变量\n",
    "    tf.summary.scalar('error', error)\n",
    "    tf.summary.histogram('weights', weights)\n",
    "    tf.summary.histogram('bias', bias)\n",
    "\n",
    "    # 合并变量\n",
    "    merged = tf.summary.merge_all()\n",
    "    # 显式初始化变量\n",
    "    init = tf.global_variables_initializer()\n",
    "    # 创建saver 对象\n",
    "    saver = tf.train.Saver()\n",
    "    #  开启会话\n",
    "    with tf.Session() as sess:\n",
    "        # 初始化变量\n",
    "        sess.run(init)\n",
    "        # 创建时间文件\n",
    "        file_writer = tf.summary.FileWriter(\"tmp/linear\", graph=sess.graph)\n",
    "        # 查看初始化模型参数之后的值\n",
    "        print(\"训练前模型参数为:w:%f.b:%f,loss:%f\" % (weights.eval(), bias.eval(), error.eval()))\n",
    "\n",
    "        # 开始驯良\n",
    "        for i in range(1000):\n",
    "            sess.run(optimizer)\n",
    "            # 运行合并变量\n",
    "            summary = sess.run(merged)\n",
    "            # 将每次迭代后的变量写入事件文件\n",
    "            file_writer.add_summary(summary, i)\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(\"第%d次训练后模型参数：w=%f,b=%f,loss=%f\" % (i + 1, weights.eval(), bias.eval(), error.eval()))\n",
    "                saver.save(sess, 'tmp/model/my_linear.ckpt')\n",
    "\n",
    "\n",
    "linear_regression()\n",
    "\n",
    "```\n",
    "\n",
    "运行以后接着可视化\n",
    "\n",
    "`tensorboard --logdir=\"tmp/linear\"`\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210405193529.png\"/>\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210405193635.png\"/>\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210405193713.png\"/>\n",
    "\n",
    "**4 . 命令行参数使用**\n",
    "\n",
    "· 1、\n",
    "tf.app.flags，它支持应用从命令行接受参数，可以用来指定集群配置等。在tf.app.flags下面有各种定义参数的类型\n",
    "\n",
    "- DEFINE_string(flag_name, default_value, docstring)\n",
    "\n",
    "- DEFINE_integer(flag_name, default_value, docstring)\n",
    "\n",
    "- DEFINE_boolean(flag_name, default_value, docstring)\n",
    "\n",
    "- DEFINE_float(flag_name, default_value,docstring)\n",
    "\n",
    "  \n",
    "\n",
    " 2、tf.app.flags,在flags有一个FLAGS标志，它在程序中可以调用到我们前面定义的flag_name\n",
    "\n",
    "3 、 通过tf.app.run() 启动main 函数\n",
    "\n",
    "代码\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# 1）定义命令行参数\n",
    "tf.app.flags.DEFINE_integer(\"max_step\", 100, \"训练模型的步数\")\n",
    "tf.app.flags.DEFINE_string(\"model_dir\", \"Unknown\", \"模型保存的路径+模型名字\")\n",
    "\n",
    "# 2）简化变量名\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "def command_demo():\n",
    "    \"\"\"\n",
    "    命令行参数演示\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"max_step:\\n\", FLAGS.max_step)\n",
    "    print(\"model_dir:\\n\", FLAGS.model_dir)\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    print(argv)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    command_demo()\n",
    "    tf.app.run()  # 会调用main函数，argv 为本文件的路径\n",
    "\n",
    "```\n",
    "\n",
    "我在命令行执行\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210405230328.png\"/>\n",
    "\n",
    "```\n",
    "max_step:\n",
    " 3\n",
    "model_dir:\n",
    " hello\n",
    "['03-命令行参数.py']\n",
    "\n",
    "```\n",
    "\n",
    "## 3 数据读取，神经网络\n",
    "\n",
    "有三种获取数据到TensorFlow程序的方法:\n",
    "1.QueueRunner:基于队列的输入管道从TensorFlow图形开头的文件中读取数据。\n",
    "2.Feeding:运行每一步时，Python代码提供数据。\n",
    "3.预加载数据:TensorFlow图中的张量包含所有数据（对于小数据集)。\n",
    "\n",
    "重点是第一种\n",
    "\n",
    "#### 3.1.1文件读取流程\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/1569451-20190619113954977-1991742319 (1).gif\"/>\n",
    "\n",
    "\n",
    "\n",
    "第一阶段构造文件名队列\n",
    "\n",
    "第二阶段读取与解码\n",
    "\n",
    "第三阶段批处理\n",
    "注:这些操作需要启动运行这些队列操作的线程，以便我们在进行文件读取的过程中能够顺利进行入队出队操作。\n",
    "\n",
    "**1 构造文件名队列**\n",
    "将需要读取的文件的文件名放入文件名队列。\n",
    "`tf.train.string_input_producer(string_tensor,shuffle=True)`\n",
    "\n",
    "- string_tensor:含有文件名+路径的1阶张量 [这里传一个文件路径列表就可以]\n",
    "- num_epochs:过几遍数据，默认无限过数据 [一直过道训练出你满意的结果，无线循环，结果会越来越拟合]\n",
    "- return文件队列\n",
    "\n",
    "**2 读取和解码**\n",
    "\n",
    "根据文件名队列读取文件并且解码\n",
    "\n",
    "**1） 读取文件内容**\n",
    "\n",
    "阅读器每次默认只读取一个样本\n",
    "\n",
    "具体来说\n",
    "\n",
    "> 文本文件默认一次读取一行\n",
    ">\n",
    "> 图片文件默认一次读取一张图片\n",
    ">\n",
    "> 二进制文件一次读取指定字节数(最好是一个样本的字节数)\n",
    ">\n",
    "> TFRecords默认一次读取一个example\n",
    "\n",
    "针对不同的文件有不同的方法\n",
    "\n",
    "- tf. TextLineReader:\n",
    "  - 阅读文本文件逗号分隔值(cSV)格式，默认按行读取\n",
    "  - return:读取器实例\n",
    "- tf.WholeFileReader:用于读取图片文件\n",
    "  - return:读取器实例\n",
    "  - tf.FixedLengthRecordReader(record_bytes):二进制文件\n",
    "  - 要读取每个记录是固定数量字节的二进制文件\n",
    "  - record_bytes:整型，指定每次读取(一个样本)的字节数o return:读取器实例\n",
    "\n",
    "- tf.TFRecordReader:读取TFRecords文件\n",
    "- return:读取器实例\n",
    "\n",
    "> 1它们有共同的读取方法:read(file_queue)，并且都会返回一个Tensors元组(key文件名字，value默认的内容(一个样本)\n",
    ">\n",
    "> 2由于默认只会读取一个样本，所以如果想要进行批处理，需要使用\n",
    "> tf.train.batch或tf.train.shuffle_batch进行批处理操作，便于之后指定每批次多个样本的训练。\n",
    "\n",
    "2)内容解码\n",
    "读取不同类型的文件，也应该对读取到的不同类型的内容进行相对应的解码操作，解码成统一的Tensor格式\n",
    "\n",
    "\n",
    "\n",
    "- tf.decode_csv:解码文本文件内容\n",
    "- tf.image.decode _jpeg(contents)\n",
    "  - 将JPEG编码的图像解码为uint8张量\n",
    "  - return:uint8张量，3-D形状[height, width, channels]\n",
    "- tf.image.decode_png(contents)\n",
    "  - 将PNG编码的图像解码为uint8张量\n",
    "  - return:张量类型，3-D形状[height, width, channels]\n",
    "- tf.decode_raw:解码二进制文件内容\n",
    "  - 与tf.FixedLengthRecordReader搭配使用，二进制读取为uint8类型\n",
    "\n",
    "> 解码阶段，默认所有的内容都解码成tf.uint8类型，如果之后需要转换成指定类型则可使用tf.cast()进行相应转换。\n",
    "\n",
    "\n",
    "\n",
    "**3批处理**\n",
    "解码之后，可以直接获取默认的一个样本内容了，但如果想要获取多个样本，需要加入到新的队列进行批处理。\n",
    "` tf.train.batch(tensors, batch_size,num_threads = 1, capacity = 32,name=None)`\n",
    "\n",
    "- 读取指定大小(个数)的张量\n",
    "- tensors:可以是包含张量的列表,批处理的内容放到列表当中\n",
    "- batch_size:从队列中读取的批处理大小\n",
    "- num_threads:进入队列的线程数\n",
    "- capacity:整数，队列中元素的最大数量\n",
    "- return:tensors\n",
    "\n",
    " `tf.train.shuffle_batch`这个方法是打乱样本顺序的其余跟上面的一样\n",
    "\n",
    "\n",
    "\n",
    "#### 3.1.2线程操作\n",
    "\n",
    "以上用到的队列都是tf.train.QueueRunner对象。\n",
    "\n",
    "每个QueueRunner都负责一个阶段，tf.train.start_queue_runners函数会要求图中的每个QueueRunner启动它的运行队列操作的线程。(这些操作需要在会话中开启)\n",
    "\n",
    "- tf.train.start_queue_runners(sess=None, coord=None)\n",
    "  - 收集图中所有的队列线程，默认同时启动线程\n",
    "  - sess:所在的会话\n",
    "  - coord:线程协调器\n",
    "  - return:返回所有线程\n",
    "- tf.train.Coordinator()\n",
    "  - 线程协调员，对线程进行管理和协调\n",
    "  - request_stop():请求停止\n",
    "  - should_stop(:询问是否结束\n",
    "  - join(threads=None, stop_grace_period_secs=120):回收线程\n",
    "  - return:线程协调员实例\n",
    "\n",
    "### 3.2 图片数据\n",
    "\n",
    "#### 3.2.1 图像的基本知识\n",
    "\n",
    "对于图像文件，我们怎么进行转换成机器学习能够理解的数据。对于图片来讲，组成图片的最基本单位是像素，所以我们获取的是每张图片的**像素值**。接触的图片有两种，一种是黑白图片，另一种是彩色图片。\n",
    "\n",
    "**1 .图片三要素**\n",
    "\n",
    "组成一张图片特征值是所有的像素值，有这么几个要素。**图片长度、图片宽度、图片通道数**。什么是图片的通道数呢，描述一个像素点，如果是灰度，那么只需要一个数值来描述它，就是单通道。如果一个像素点，有RGB三种颜色来描述它，就是三通道。那所以\n",
    "\n",
    "- 灰度图片：单通道\n",
    "- 彩色图片：三通道\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210406003847.png\"/>\n",
    "\n",
    "> 假设一张彩色图片的长200，宽200，通道数为3，那么总的像素数量为200x200x3\n",
    "\n",
    "**2 张量形状**\n",
    "\n",
    "读取图片之后，怎么用张量形状来表示呢。一张图片就是一个`3D`张量，[height, width, channel]，height就表示高，width表示宽，channel表示通道数。我们会经常遇到`3D`和`4D`的表示\n",
    "\n",
    "- 单个图片：[height, width, channel]\n",
    "- 多个图片（4D）：[batch, height, width, channel]，batch表示批数量\n",
    "\n",
    "**3 图片特征值处理**\n",
    "\n",
    "在进行图片识别的时候，每个图片样本的特征数量要保持相同（方便神经网络的训练）。所以需要将所有图片张量大小统一转换。另一方面如果图片的像素量太大，也可以通过这种方式适当减少像素的数量，减少训练的计算开销\n",
    "\n",
    "- tf.image.resize_images(images, size)\n",
    "  - 缩小放大图片\n",
    "  - images：4-D形状[batch, height, width, channels]，或3-D形状的张量[height, width, channels]的图片数据\n",
    "  - size：1-D int32张量：new_height, new_width，图像的新尺寸\n",
    "  - 返回4-D格式或者3-D格式图片\n",
    "\n",
    "**4 数据格式**\n",
    "\n",
    "- 存储：uint8(节约空间)\n",
    "- 矩阵计算：float32(提高精度)\n",
    "\n",
    "#### 3.2.2 狗图片读取案例\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "def picture_read(filename_list):\n",
    "    # 1. 构造文件名队列\n",
    "    file_queue = tf.train.string_input_producer(filename_list)\n",
    "\n",
    "    # 2. 读取与解码\n",
    "    reader = tf.WholeFileReader()\n",
    "    # key 文件名 value 一张图片的原始编码形式\n",
    "    key, value = reader.read(file_queue)\n",
    "\n",
    "    # 解码阶段\n",
    "    image = tf.image.decode_jpeg(value)\n",
    "    # 图像的形状类型修改,改成统一的宽高\n",
    "    image_resized = tf.image.resize_images(image, [200, 200])\n",
    "    # 静态形状修改，添加通道数\n",
    "    image_resized.set_shape(shape=[200, 200, 3])\n",
    "    image_batch = tf.train.batch([image_resized], batch_size=10, num_threads=1, capacity=100)\n",
    "    print(image_batch)\n",
    "    with tf.Session() as sess:\n",
    "        # 开启县城\n",
    "        # 线程协调员\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        print(image_resized.shape)\n",
    "        print(image_resized.eval())\n",
    "        # 回收线程\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 构造路径+ 文件名列表\n",
    "    filename = os.listdir(\"dog\")\n",
    "\n",
    "    filename_list = [os.path.join('dog', i) for i in filename]\n",
    "    picture_read(filename_list)\n",
    "\n",
    "```\n",
    "\n",
    "### 3.3 二进制文件\n",
    "\n",
    "#### 3.3.1 CIFAR10二进制数据集介绍\n",
    "\n",
    "CIFAR-10数据集由10个类的60000个32x32彩色图像组成，每个类有6000个图像。\n",
    "\n",
    "有50000个训练图像和10000个测试图像。\n",
    "\n",
    "数据集分为五个训练批次和一个测试批次，每个批次有10000个图像。测试批次包含来自每个类的恰好1000个随机选择的图像。类的图像比另一个更多。在他们之间，训练批次包含来自每个类的正好5000张图像。\n",
    "以下是数据集中的类，以及来自每个类的10个随机图像:\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210406012605.png\"/>\n",
    "\n",
    "- 二进制版本数据文件\n",
    "\n",
    "二进制版本包含文件data_batch_1.bin，data_batch_2.bin，...，data_batch_5.bin以及test_batch.bin。\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210406013226.png\"/>\n",
    "\n",
    "这些文件中的每一个格式如下，数据中每个样本包含了目标值和特征值：\n",
    "\n",
    "> ```\n",
    "> <1×标签> <3072×像素> \n",
    "> ... \n",
    "> <1×标签> <3072×像素>\n",
    "> ```\n",
    "\n",
    "第一个字节是第一个图像的标签，它是一个0-9范围内的数字。接下来的3072个字节是图像像素的值。前1024个字节是红色通道值，下1024个绿色，最后1024个蓝色。值以行优先顺序存储，因此前32个字节是图像第一行的红色通道值。 每个文件都包含10000个这样的3073字节的“行”图像，但没有任何分隔行的限制。因此每个文件应该完全是30730000字节长。\n",
    "\n",
    "#### 3.2.2 CIFAR10 二进制数据读取 案例\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "class Cifar():\n",
    "\n",
    "    def __init__(self):\n",
    "        # 设置图像大小\n",
    "        self.height = 32\n",
    "        self.width = 32\n",
    "        self.channel = 3\n",
    "\n",
    "        # 设置图像字节数 一张图片大小 是 3*32*32\n",
    "        self.image = self.height * self.width * self.channel\n",
    "        self.label = 1\n",
    "        self.sample = self.image + self.label\n",
    "\n",
    "    def read_and_decode(self, file_list):\n",
    "        # 1. 构造文件名队列\n",
    "        file_queue = tf.train.string_input_producer(file_list)\n",
    "\n",
    "        # 2. 读取与解码\n",
    "        reader = tf.FixedLengthRecordReader(self.sample)\n",
    "        # key 文件名  value 一个样本\n",
    "        key, value = reader.read(file_queue)\n",
    "        # 解码阶段\n",
    "        decoded = tf.decode_raw(value, tf.uint8)  # 得到一张图片的label数字和三通道数字 [1+ 3*32*32] 一维张量\n",
    "        # 把目标值和特征值切开\n",
    "        label = tf.slice(decoded, [0], [self.label])\n",
    "        image = tf.slice(decoded, [self.label], [self.image])\n",
    "        # 调整图片形状\n",
    "        image_reshaped = tf.reshape(image, shape=[self.channel, self.height, self.width])  # 3*32*32\n",
    "        # 但是tensorflow 的世界里想要 32*32*3这种形式的图片  我们要把这个三维张量的坐标轴转换一下\n",
    "        image_transposed = tf.transpose(image_reshaped, [1, 2, 0])  # 32*32 *3\n",
    "        # 调整图像类型\n",
    "        image_cast = tf.cast(image_transposed, dtype=tf.float32)\n",
    "\n",
    "        # 3.批处理\n",
    "        label_batch, image_batch = tf.train.batch([label, image_cast], batch_size=100, num_threads=1, capacity=100)\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "            key_new, value_new = key.eval(), value.eval()\n",
    "            print(\"key_new:\\n\", key_new)\n",
    "            print(\"value_new:\\n\", value_new)\n",
    "            label_value, image_value = label_batch.eval(), image_batch.eval()\n",
    "            # 回收线程\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "\n",
    "        return label_value, image_value\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cifar = Cifar()\n",
    "    file_name = os.listdir('cifar-10-batches-bin')\n",
    "    # 构造问及高明路径列表\n",
    "    file_list = [os.path.join('cifar-10-batches-bin', file) for file in file_name if file[-3:] == 'bin']\n",
    "\n",
    "    cifar.read_and_decode(file_list)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "### 3.4 TFRecords\n",
    "\n",
    "#### 3.4.1 什么是TFRecords文件\n",
    "\n",
    "TFRecords其实是一种二进制文件，虽然它不如其他格式好理解，但是它能更好的利用内存，更方便复制和移动，并且不需要单独的标签文件。\n",
    "\n",
    "使用步骤︰\n",
    "\n",
    "1）获取数据\n",
    "2）将数据填入到 Example协议内存块(protocol buffer)\n",
    "3）将协议内存块序列化为字符串，并且通过 tf.python_io.TFRecordwriter写入到TFRecords文件。\n",
    "\n",
    "- 文件格式 *.tfrecords\n",
    "\n",
    "#### 3.4.2 Example结构解析\n",
    "\n",
    "这里还以刚才的cifar10 数据集举例\n",
    "\n",
    "```\n",
    "Example:\n",
    "features {feature {\n",
    "key: \"image\"value {\n",
    "bytes_list {\n",
    "value: \"\\377\\374\\375\\3721356\\351\\365\\361\\350\\356\\352\\350'}\n",
    "}}\n",
    "feature {\n",
    "key: \"label\"value {\n",
    "int64_list {value: 9}\n",
    "}}}\n",
    "\n",
    "```\n",
    "\n",
    "这是一个样本，样本里有两个feature，每个feature都有key 和value\n",
    "\n",
    "\n",
    "\n",
    "- `tf.train.Example` 协议内存块(protocol buffer)(协议内存块包含了字段 `Features`)\n",
    "- `Features`包含了一个f`eature`字段\n",
    "- **f`eature`中包含要写入的数据、并指明数据类型**。这是一个样本的结构，**批数据**需要**循环存入**这样的结构\n",
    "\n",
    "```python\n",
    " cifar10\n",
    " 特征值 - image - 3072个字节\n",
    " 目标值 - label - 1个字节\n",
    "\n",
    "# 创建样本\n",
    "example = tf.train.Example(features=tf.train.Features(feature={\n",
    "    \"image\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),\n",
    "    \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
    "                }))\n",
    "    \n",
    "# 序列化\n",
    "example.SerializeToString()\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "- tf.train.Example(features=None)\n",
    "  - 写入tfrecords文件\n",
    "  - features:tf.train.Features类型的特征实例\n",
    "  - return：example格式协议块\n",
    "- tf.train.Features(feature=None)\n",
    "  - 构建每个样本的信息键值对\n",
    "  - feature: 字典数据, key为要保存的名字\n",
    "  - value为tf.train.Feature实例\n",
    "  - return:Features类型\n",
    "- tf.train.Feature(options)\n",
    "  - options：例如\n",
    "    - bytes_list=tf.train. BytesList(value=[Bytes])\n",
    "    - int64_list=tf.train. Int64List(value=[Value])\n",
    "  - 支持存入的类型如下\n",
    "  - tf.train.Int64List(value=[Value])\n",
    "  - tf.train.BytesList(value=[Bytes]) \n",
    "  - tf.train.FloatList(value=[value])\n",
    "\n",
    "上面三个api 是层层嵌套使用的。实在不好理解可以照着抄后面案例代码帮助理解\n",
    "\n",
    "#### 3.4.3 案例：CIFAR10数据，存入TFRecords文件\n",
    "\n",
    "**分析**\n",
    "\n",
    "- 构造存储实例，tf.python_io.TFRecordWriter(path)\n",
    "  - 写入tfrecords文件\n",
    "  - path: TFRecords文件的路径 + 文件名字\n",
    "  - return：写文件\n",
    "  - method\n",
    "  - write(record): 向文件中写入一个example\n",
    "  - close(): 关闭文件写入器\n",
    "- 循环将数据填入到`Example`协议内存块(protocol buffer)\n",
    "\n",
    "**代码**\n",
    "\n",
    "上面我们读取到了cifar10 文件接下来吧读取的cifar10 文件传到到tfrecords里面\n",
    "\n",
    "```python\n",
    "def write_to_trrecords(self, label_batch, image_batch):\n",
    "\n",
    "    with tf.python_io.TFRecordWriter(\"cifar10.tfrecords\") as writer:\n",
    "        # 循环构造example对象，病序列化写入文件\n",
    "        for i in range(100): # batch_size 是100 要把所有数据遍历\n",
    "            image = image_batch[i].tostring()\n",
    "            label = label_batch[i][0]  # label_batch[i] = [9] 是一维张量，要把标签数字取出\n",
    "\n",
    "            example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                \"image\": tf.train.Feature(bytes_list=tf.train.BytesList(value=[image])),\n",
    "                \"label\": tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
    "            }))\n",
    "            writer.write(example.SerializeToString())\n",
    "  \n",
    "```\n",
    "\n",
    "main函数改动\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    cifar = Cifar()\n",
    "    file_name = os.listdir('cifar-10-batches-bin')\n",
    "    # 构造问及高明路径列表\n",
    "    file_list = [os.path.join('cifar-10-batches-bin', file) for file in file_name if file[-3:] == 'bin']\n",
    "\n",
    "    label_batch, image_batch = cifar.read_and_decode(file_list)\n",
    "    cifar.write_to_trrecords(label_batch, image_batch)\n",
    "```\n",
    "\n",
    "运行结果\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210406141807.png\"/>\n",
    "\n",
    "#### 3.4.4 读取TFRecords文件\n",
    "\n",
    "读取这种文件整个过程与其他文件一样，**只不过需要有个解析Example的步骤**。\n",
    "\n",
    "从TFRecords文件中读取数据， 可以使用`tf.TFRecordReader`的**`tf.parse_single_example`解析器**。\n",
    "\n",
    "这个操作可以**将`Example`协议内存块**(protocol buffer)解析为**张量**。\n",
    "\n",
    "```python\n",
    "# 多了解析example的一个步骤\n",
    "feature = tf.parse_single_exmple(values, features = {\n",
    "    \"image\":tf.FixedLenFeature([],tf.string),\n",
    "    \"label\":tf.FixedLenFeature([],tf.int64)\n",
    "})\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "- **tf.parse_single_example(serialized, features=None, name=None)**\n",
    "  - 解析一个单一的Example原型\n",
    "  - **serialized：标量字符串Tensor，一个序列化的Example**\n",
    "  - features：dict字典数据，键为读取的名字，值为FixedLenFeature\n",
    "  - return:一个键值对组成的字典，键为读取的名字\n",
    "- **tf.FixedLenFeature(shape, dtype)**\n",
    "  - shape：输入数据的形状，一般不指定, 为空列表\n",
    "  - dtype：**输入数据类型，与存储进文件的类型要一致**\n",
    "  - **类型只能是float32, int64, string**\n",
    "\n",
    "#### 3.4 5案例：读取CIFAR的TFRecords文件\n",
    "\n",
    " **分析**\n",
    "\n",
    "- 使用tf.train.string_input_producer构造文件队列\n",
    "- tf.TFRecordReader 读取TFRecords数据并进行解析\n",
    "  - **tf.parse_single_example进行解析**\n",
    "- tf.decode_raw解码\n",
    "  - **类型是bytes类型需要解码**\n",
    "  - **其他类型不需要**\n",
    "- **处理图片数据形状以及数据类型**，批处理返回\n",
    "- 开启会话线程运行\n",
    "\n",
    " **代码**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def read_tfrecords():\n",
    "    # 1. 构造文件名队列\n",
    "    file_queue = tf.train.string_input_producer([\"cifar10.tfrecords\"])\n",
    "\n",
    "    # 读取和解码\n",
    "    # 读取\n",
    "    reader = tf.TFRecordReader()\n",
    "    key, value = reader.read(file_queue)\n",
    "\n",
    "    # 解析example\n",
    "\n",
    "    feature = tf.parse_single_example(value, features={\n",
    "        \"image\": tf.FixedLenFeature([], tf.string),\n",
    "        \"label\": tf.FixedLenFeature([], tf.int64)  # 这里的数据类型要与之前创建的时候保持一直\n",
    "    })\n",
    "\n",
    "    image = feature[\"image\"]  # 图片是二进制，要解码成张量[32,32,3]\n",
    "\n",
    "    label = feature[\"label\"]\n",
    "\n",
    "    # 解码成一维张量\n",
    "    image_decode = tf.decode_raw(image, tf.float32) # 这里的数据类型一定要与之前一致是tf.float32 教程里是uint 8 搞混了，总之一定要跟以前一样，否则形状转化不来\n",
    "\n",
    "    # 改变形状\n",
    "    img_reshaped = tf.reshape(image_decode, [32, 32, 3])\n",
    "    # 解码\n",
    "     # 3. 构造批处理队列\n",
    "    image_batch, label_batch = tf.train.batch([img_reshaped, label], batch_size=2, num_threads=2, capacity=100)\n",
    "    print(\"image_batch\", image_batch)\n",
    "    print('label_batch:', label_batch)\n",
    "    # 开启回话\n",
    "    with tf.Session() as sess:\n",
    "        # 开启县城\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        for i in range(4):\n",
    "        # print(img_reshaped.eval())\n",
    "            print('第%d批label:'%(i+1),label_batch.eval())\n",
    "        # 回收资源\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "image_batch Tensor(\"batch:0\", shape=(100, 32, 32, 3), dtype=float32)\n",
    "label_batch: Tensor(\"batch:1\", shape=(100,), dtype=int64)\n",
    "\n",
    "\n",
    "第1批label: [6 8]\n",
    "第2批label: [8 3]\n",
    "第3批label: [4 6]\n",
    "第4批label: [0 6]\n",
    "```\n",
    "\n",
    "### 3.5 神经网络基础\n",
    "\n",
    "### 3.6 神经网络原理\n",
    "\n",
    "这两帕我觉得没什么好说的 跳过了\n",
    "\n",
    "### 3.7 minist 手写数字识别\n",
    "\n",
    "#### 3.7.1 数据集介绍\n",
    "\n",
    "偷懒直接截图了\n",
    "\n",
    "<img src=\"https://gitee.com/xu_kai_xuyouqian/picture-bed/raw/master/20210406183637.png\"/>\n",
    "\n",
    "详细情况可以转到\n",
    "\n",
    "https://www.jianshu.com/p/d282bce1a999\n",
    "\n",
    "### 3.7.2 数据api\n",
    "\n",
    "TensorFlow框架自带了获取这个数据集的接口，所以不需要自行读取。\n",
    "\n",
    "- from tensorflow.examples.tutorials.mnist import input_data\n",
    "  -  mnist = input_data.read_data_sets(path, one_hot=True)\n",
    "     - mnist.train.next_batch(100)(提供批量获取功能)\n",
    "     - mnist.train.images、labels\n",
    "     - mnist.test.images、labels\n",
    "\n",
    "\n",
    "\n",
    "**代码**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "def full_connection():\n",
    "    # 1. 准备数据\n",
    "    mnist = input_data.read_data_sets('mnist_data', one_hot=True)\n",
    "    # 之前说了三种数据读取方式，浙西用第二种\n",
    "    # 二、Feeding:运行每一步时，Python代码提供数据。\n",
    "    # 图片的数据，32*32 像素单通道 784 None先不确定 表示图片的张数\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, 784])\n",
    "    y_true = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n",
    "\n",
    "    # 2. 构建模型\n",
    "    # 图片矩阵是 [batch_size,784] weight 形状是[784,10] 最后点乘是 [batch_size,10]\n",
    "    weights = tf.Variable(initial_value=tf.random_uniform(shape=[784, 10]))\n",
    "    bias = tf.Variable(initial_value=tf.random_uniform(shape=[10]))  # 偏置[10]跟可以通过广播跟 [batch_size,10]相加\n",
    "    y_predict = tf.matmul(x, weights) + bias\n",
    "\n",
    "    # 3.构造损失函数 分类任务，这里用交叉信息熵\n",
    "    error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict))\n",
    "\n",
    "    # 4. 优化损失\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(error)\n",
    "\n",
    "    # 初始化变量\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # 开启会话\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        image, label = mnist.train.next_batch(100)\n",
    "        print(\"训练之前损失为%f\" % sess.run(error, feed_dict={x: image, y_true: label}))\n",
    "\n",
    "        # 开始训练\n",
    "        for i in range(100):\n",
    "            _, loss = sess.run([optimizer, error], feed_dict={x: image, y_true: label})\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(\"第%d次训练,损失为%f\" % (i + 1, loss))\n",
    "\n",
    "\n",
    "full_connection()\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "训练之前损失为4.433406\n",
    "第10次训练,损失为4.015255\n",
    "第20次训练,损失为3.664671\n",
    "第30次训练,损失为3.389782\n",
    "第40次训练,损失为3.164690\n",
    "第50次训练,损失为2.974276\n",
    "第60次训练,损失为2.808177\n",
    "第70次训练,损失为2.659248\n",
    "第80次训练,损失为2.523003\n",
    "第90次训练,损失为2.396847\n",
    "第100次训练,损失为2.279329\n",
    "```\n",
    "\n",
    "优化，加上准确率计算，和存储模型\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "\n",
    "\n",
    "def full_connection():\n",
    "    # 1. 准备数据\n",
    "    mnist = input_data.read_data_sets('mnist_data', one_hot=True)\n",
    "    # 之前说了三种数据读取方式，浙西用第二种\n",
    "    # 二、Feeding:运行每一步时，Python代码提供数据。\n",
    "    # 图片的数据，32*32 像素单通道 784 None先不确定 表示图片的张数\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, 784])\n",
    "    y_true = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n",
    "\n",
    "    # 2. 构建模型\n",
    "    # 图片矩阵是 [batch_size,784] weight 形状是[784,10] 最后点乘是 [batch_size,10]\n",
    "    weights = tf.Variable(initial_value=tf.random_uniform(shape=[784, 10]))\n",
    "    bias = tf.Variable(initial_value=tf.random_uniform(shape=[10]))  # 偏置[10]跟可以通过广播跟 [batch_size,10]相加\n",
    "    y_predict = tf.matmul(x, weights) + bias\n",
    "\n",
    "    # 3.构造损失函数 分类任务，这里用交叉信息熵\n",
    "    error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict))\n",
    "\n",
    "    # 4. 优化损失\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(error)\n",
    "\n",
    "    # 5. 计算准确率\n",
    "    bool_list = tf.equal(tf.argmax(y_true, axis=1), tf.argmax(y_predict, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(bool_list, tf.float32))\n",
    "\n",
    "    # 创建saver 对象\n",
    "    saver = tf.train.Saver()\n",
    "    # 初始化变量\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    init_loss = float('inf')\n",
    "\n",
    "    # 开启会话\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        image, label = mnist.train.next_batch(100)\n",
    "        print(\"训练之前损失为%f\" % sess.run(error, feed_dict={x: image, y_true: label}))\n",
    "\n",
    "        # 开始训练\n",
    "        for i in range(100):\n",
    "            _, loss = sess.run([optimizer, error], feed_dict={x: image, y_true: label})\n",
    "            if loss < init_loss:\n",
    "                init_loss = loss\n",
    "                saver.save(sess, 'tmp/model/mnist.ckpt')\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(\"第%d次训练,损失为%f\" % (i + 1, loss))\n",
    "\n",
    "\n",
    "full_connection()\n",
    "\n",
    "```\n",
    "\n",
    "### 4. 卷积神经网络\n",
    "\n",
    "这里感觉讲的其实不太好，推荐看 李宏毅讲的cnn，不过这里讲了一些更详细的卷积api 可以先看李宏毅的再看这里的，这里更着重讲代码，原理部分可以用李宏毅的视频代替。笔记我也不再整理了，直接写代码\n",
    "\n",
    "https://www.bilibili.com/video/BV1JE411g7XF?p=17\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# 定义命令行参数\n",
    "tf.app.flags.DEFINE_integer('is_train', 1, \"指定是否是训练模型，还是拿数据去预测\")\n",
    "# 简化变量名\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "\n",
    "def create_weights(shape):\n",
    "    return tf.Variable(initial_value=tf.random_normal(shape=shape, stddev=0.01))  # 正态分布的标准差是0.01\n",
    "\n",
    "\n",
    "def create_model(x):\n",
    "    '''\n",
    "    构建卷积神经网络\n",
    "    :param x:\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    # 1) 第一个卷积大层\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        # 卷积层\n",
    "        # 将x[None,784]姓黄进行修改\n",
    "        input_x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "        # 定义filter 和偏置\n",
    "        conv1_weights = create_weights(shape=[5, 5, 1, 32])  # 1表示输入的通道数，32表示输出的通道数\n",
    "        conv1_bias = create_weights(shape=[32])\n",
    "\n",
    "        conv1_x = tf.nn.conv2d(input=input_x, filter=conv1_weights, strides=[1, 1, 1, 1], padding=\"SAME\") + conv1_bias\n",
    "\n",
    "        # conv1_x = [batch_size,14,14,32] 最后一个32是通道\n",
    "        # 激活层\n",
    "        relu1_x = tf.nn.relu(conv1_x)\n",
    "\n",
    "        # 池化层\n",
    "        pool1_x = tf.nn.max_pool(value=relu1_x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1],\n",
    "                                 padding=\"SAME\")  # 因为不需要在batch_size 和channel坐标轴上做池化\n",
    "\n",
    "        # pool1_x = [batch_size,14,14,32]\n",
    "    # 2） 第二个卷积大层\n",
    "    with tf.variable_scope('conv2'):\n",
    "        # 卷积层\n",
    "        # 定义filter 和偏置\n",
    "        conv2_weights = create_weights(shape=[5, 5, 32, 64])\n",
    "        conv2_bias = create_weights(shape=[64])\n",
    "        conv2_x = tf.nn.conv2d(input=pool1_x, filter=conv2_weights, strides=[1, 1, 1, 1], padding='SAME') + conv2_bias\n",
    "        # conv2_x  = [batch_size,14,14,64]\n",
    "\n",
    "        # 激活层\n",
    "        relu2_x = tf.nn.relu(conv2_x)\n",
    "\n",
    "        # 池化层\n",
    "        pool2_x = tf.nn.max_pool(value=relu2_x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # pool2_x = [batch_size,7,7,64]\n",
    "\n",
    "    # 3) 全连接层\n",
    "    with tf.variable_scope(\"full_connection\"):\n",
    "        '''\n",
    "        [batch_size,7,7,64] 重构形状 [batch_size,7*7*64] \n",
    "        降维 [batch_size,10] \n",
    "        '''\n",
    "        x_fc = tf.reshape(pool2_x, shape=[-1, 7 * 7 * 64])\n",
    "        weights_fc = create_weights(shape=[7 * 7 * 64, 10])\n",
    "        bias_fc = create_weights(shape=[10])\n",
    "        y_predict = tf.matmul(x_fc, weights_fc) + bias_fc\n",
    "\n",
    "    return y_predict\n",
    "\n",
    "\n",
    "def full_connection():\n",
    "    # 1. 准备数据\n",
    "    mnist = input_data.read_data_sets('mnist_data', one_hot=True)\n",
    "    # 之前说了三种数据读取方式，浙西用第二种\n",
    "    # 二、Feeding:运行每一步时，Python代码提供数据。\n",
    "    # 图片的数据，32*32 像素单通道 784 None先不确定 表示图片的张数\n",
    "    x = tf.placeholder(dtype=tf.float32, shape=[None, 784])\n",
    "    y_true = tf.placeholder(dtype=tf.float32, shape=[None, 10])\n",
    "\n",
    "    y_predict = create_model(x)\n",
    "\n",
    "    # 3.构造损失函数 分类任务，这里用交叉信息熵\n",
    "    error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true, logits=y_predict))\n",
    "\n",
    "    # 4. 优化损失\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(error)\n",
    "\n",
    "    # 5. 计算准确率\n",
    "    bool_list = tf.equal(tf.argmax(y_true, axis=1), tf.argmax(y_predict, axis=1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(bool_list, tf.float32))\n",
    "\n",
    "    # 创建saver 对象\n",
    "    saver = tf.train.Saver()\n",
    "    # 初始化变量\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    init_loss = float('inf')\n",
    "\n",
    "    # 开启会话\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        if FLAGS.is_train == 1:\n",
    "            image, label = mnist.train.next_batch(100)\n",
    "            print(\"训练之前损失为%f\" % sess.run(error, feed_dict={x: image, y_true: label}))\n",
    "\n",
    "            # 开始训练\n",
    "            for i in range(100):\n",
    "                _, loss = sess.run([optimizer, error], feed_dict={x: image, y_true: label})\n",
    "                if loss < init_loss:\n",
    "                    init_loss = loss\n",
    "                    saver.save(sess, 'tmp/model/mnist.ckpt')\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(\"第%d次训练,损失为%f\" % (i + 1, loss))\n",
    "        else:\n",
    "            # 如果不是训练，我们就去进行预测测试集数据\n",
    "            for i in range(100):\n",
    "                # 每次拿一个样本预测\n",
    "                mnist_x, mnist_y = mnist.test.next_batch(1)\n",
    "                print(\"第%d个样本的真实值为：%d, 模型预测结果为：%d\" % (\n",
    "                    i + 1,\n",
    "                    tf.argmax(sess.run(y_true, feed_dict={x: mnist_x, y_true: mnist_y}), 1).eval(),\n",
    "                    tf.argmax(sess.run(y_predict, feed_dict={x: mnist_x, y_true: mnist_y}), 1).eval()\n",
    "                )\n",
    "                      )\n",
    "\n",
    "\n",
    "full_connection()\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbadeca5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}